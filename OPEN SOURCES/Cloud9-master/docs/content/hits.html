<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
    "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta name="generator" content=
  "HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org" />
  <meta http-equiv="Content-Type" content="text/html; charset=us-ascii" />

  <title>Cloud9: A MapReduce Library for Hadoop</title>
  <style type="text/css" media="screen">
/*<![CDATA[*/
  @import url( ../style.css );
  /*]]>*/
  </style>
</head>

<body>
  <div id="wrap">
    <div id="container" class="one-column">
      <!-- header START -->

      <div id="header">
        <div id="caption">
          <h1 id="title" style="color: white;">Cloud<sup><small>9</small></sup></h1>

          <div id="tagline">
            A MapReduce Library for Hadoop
          </div>
        </div>

        <div class="fixed"></div>
      </div><!-- header END -->
      <!-- navigation START -->

      <div id="navigation">
        <script type="text/javascript" src="menu.js">
</script>

        <div class="fixed"></div>
      </div><!-- navigation END -->
      <!-- content START -->

      <div id="content">
        <!-- main START -->

        <div id="main">
          <!--- START MAIN CONTENT HERE -->

          <h2>Design Patterns &amp; Algorithms &raquo; HITS</h2>

          <div class="post">
            <div class="content">
              <p>Cloud<sup><small>9</small></sup> contains a MapReduce implementation of
              <a href="http://www.cs.cornell.edu/home/kleinber/auth.pdf">Kleinberg's Hubs
              and Authorities algorithm</a>.</p>

              <p>Please see the <a href=
              "./pagerank.html">Cloud<sup><small>9</small></sup>PageRank
              documentation</a> for a description of the link graph structure used in
              Cloud<sup><small>9</small></sup>. As with PageRank, the HITS code was
              developed and tested against the ClueWeb09 link graph, but can also be run
              against the Wikipedia link graph. The PageRank implementation page <a href=
              "./pagerank.html">describes</a> how to extract this link graph.</p>

              <p>The rest of this guide assumes you are working with the ClueWeb09 link
              graph (as discussed in the PageRank paper). Here are the relevant driver
              classes for running HITS:</p>

              <ul>
                <li><code>edu.umd.cloud9.example.hits.AFormatterWG</code></li>

                <li><code>edu.umd.cloud9.example.hits.HFormatterWG</code></li>

                <li><code>edu.umd.cloud9.example.hits.MergeFormattedRecords</code></li>

                <li><code>edu.umd.cloud9.example.hits.PartitionGraph</code></li>

                <li><code>edu.umd.cloud9.example.hits.HubsAndAuthorities</code></li>

                <li>
                <code>edu.umd.cloud9.example.hits.HubsAndAuthoritiesSchimmy</code></li>
              </ul>

              <p>Below, we present step-by-step instructions for running the various
              experiments.</p>

              <h4>Preparing the input data</h4>

              <h5>Format</h5>

              <p>First, we take the plain-text graph structure and pack it into the
              appropriate Hadoop data structures. This is accomplished using three driver
              programs.</p>

              <p>The first driver program
              <code>edu.umd.cloud9.example.hits.AFormatterWG</code>
              (<b>A</b>uthority<b>FormatterW</b>eb<b>G</b>raph) constructs a serialized
              list of nodes and their incoming links from the plain-text graph structure.
              It takes five command-line arguments:</p>

              <ul>
                <li>[input-path]: input directory</li>

                <li>[output-path]: output directory</li>

                <li>[num-mappers]: number of mappers to use (may be overridden by
                Hadoop)</li>

                <li>[num-reducers]: number of reducers to use, also the number of output
                files</li>

                <li>[stoplist-path]: location of the stoplist file (must be provided,
                <a href="#stoplists">see below</a>)</li>
              </ul>

              <p>Here is a sample command-line invocation:</p>
              <pre>
hadoop jar cloud9.jar edu.umd.cloud9.example.hits.AFormatterWG \
     /shared/Wikipedia/graph/en20100130/ /tmp/mmcgrath/wiki/outa 500 500 /tmp/mmcgrath/wiki/toobig.txt
</pre>

              <p>The second driver program
              <code>edu.umd.cloud9.example.hits.HFormatterWG</code>
              (<b>H</b>ub<b>FormatterW</b>eb<b>G</b>raph) constructs a serialized list of
              nodes and their outgoing links from the plain-text graph structure. It
              takes the same five command-line arguments:</p>

              <ul>
                <li>[input-path]: input directory</li>

                <li>[output-path]: output directory</li>

                <li>[num-mappers]: number of mappers to use (may be overridden by
                Hadoop)</li>

                <li>[num-reducers]: number of reducers to use, also the number of output
                files</li>

                <li>[stoplist-path]: location of the stoplist file (must be provided,
                <a href="#stoplists">see below</a>)</li>
              </ul>

              <p>Here is a sample command-line invocation:</p>
              <pre>
hadoop jar cloud9.jar edu.umd.cloud9.example.hits.HFormatterWG \
     /shared/Wikipedia/graph/en20100130/ /tmp/mmcgrath/wiki/outh 500 500 /tmp/mmcgrath/wiki/toobig.txt
</pre>

              <h5><a name="stoplists" id="stoplists">Stoplists</a></h5>

              <p>It may be desirable to exclude certain highly-linked nodes from the rank
              calcuations in order to prevent them from slowing down execution of the
              Hadoop job. You may supply the <code>AFormatterWG</code> and the
              <code>HFormatterWG</code> a list of nodeIDs to ignore in a text file, with
              one nodeID listed per line. If you do not wish to ignore any nodes, simply
              supply an empty file for this parameter. A sample stoplist file for
              ClueWeb09 segment one can be seen <a href="./toobig.txt">here</a>.</p>

              <h5>Merge</h5>

              <p>The third driver program merges the output of the previous two jobs into
              one set of output files. It takes five command line arguments:</p>

              <ul>
                <li>[hub-input-path]: input directory containing output of
                HFormatterWG</li>

                <li>[auth-input-path]: input directory containing output of
                AFormatterWG</li>

                <li>[output-path]: output directory</li>

                <li>[num-mappers]: number of mappers to use (may be overridden by
                Hadoop)</li>

                <li>[num-reducers]: number of reducers to use, also the number of output
                files</li>
              </ul>

              <p>Here is a sample command-line invocation:</p>
              <pre>
hadoop jar cloud9.jar
edu.umd.cloud9.example.hits.MergeFormattedRecords \
    /tmp/mmcgrath/wiki/outh /tmp/mmcgrath/wiki/outa /tmp/mmcgrath/wiki/merged 500 500
</pre>

              <h5>Partition</h5>

              <p>Finally, we need to partition the graph. The appropriate driver program
              is <code>edu.umd.cloud9.example.hits.PartitionGraph</code>, which takes the
              following command-line arguments:</p>

              <ul>
                <li>[inputDir]: input directory</li>

                <li>[outputDir]: output directory</li>

                <li>[numPartitions]: number of partitions</li>

                <li>[useRange?]: 1 to user range partitioning or 0 otherwise</li>

                <li>[nodeCount]: number of nodes in the graph</li>
              </ul>

              <p>So, let's partition the graph both ways:</p>
              <pre>
# hash partition the graph
hadoop fs -mkdir /tmp/mmcgrath/hits-hash

hadoop jar cloud9.jar edu.umd.cloud9.example.hits.PartitionGraph \
   /umd-lin/jimmylin/PageRankRecords /tmp/mmcgrath/hits-hash/iter0000 100 0 50220423

# range partition the graph
hadoop fs -mkdir /tmp/mmcgrath/hits-range

hadoop jar cloud9.jar edu.umd.cloud9.example.hits.PartitionGraph \
   /umd-lin/jimmylin/PageRankRecords /tmp/mmcgrath/hits-range/iter0000 100 1 50220423
</pre>

              <p>We want to specify an output directory of the form
              <code>/base/path/iter0000</code> because that's the convention of the HITS
              driver program: the output of iteration <i>x</i> is stored in a
              subdirectory named <code>iterXXXX</code>.</p>

              <h4>Running HITS</h4>

              <p>Once the input data is formatted properly, we can run HITS. The two HITS
              driver programs are:</p>

              <ul>
                <li><code>edu.umd.cloud9.example.hits.HubsAndAuthorities</code> for the
                basic implementation: shuffles graph structure from iteration to
                iteration.</li>

                <li><code>edu.umd.cloud9.example.hits.HubsAndAuthoritiesSchimmy</code>
                for the Schimmy implementation: avoids graph structure shuffling.</li>
              </ul>

              <p>Both driver programs take the same command-line arguments:</p>

              <ul>
                <li>[basePath]: the base path</li>

                <li>[numNodes]: number of nodes in the graph</li>

                <li>[start]: starting iteration</li>

                <li>[end]: ending iteration</li>

                <li>[useCombiner?]: 1 for using combiner, 0 for not</li>

                <li>[useInMapCombiner?]: 1 for using in-mapper combining, 0 for not</li>

                <li>[useRange?]: 1 for range partitioning, 0 for not</li>

                <li>[num Mappers]: number of mappers to use</li>

                <li>[numReducers]: number of reducers to use. This should remain constant
                between iterations</li>
              </ul>

              <p>Some notes:</p>

              <ul>
                <li>The starting and ending iterations will correspond to paths
                <code>/base/path/iterXXXX</code> and <code>/base/path/iterYYYY</code>. As
                a example, if you specify 0 and 10 as the starting and ending iterations,
                the driver program will start with the graph structure stored at
                <code>/base/path/iter0000</code>; final results will be stored at
                <code>/base/path/iter0010</code>.</li>

                <li>As with PageRank, the driver program will allow you to use both
                combiners and the in-mapper combining pattern, but using both actually
                slows down the algorithm. So those bits should be considered
                exclusive.</li>
              </ul>

              <p>As in the PageRank, examples here is the command-line invocation of the
              "best-practices" baseline:</p>
              <pre>
# hash partitioning, basic implementation, +combiner
hadoop jar cloud9.jar edu.umd.cloud9.example.hits.HubsAndAuthorities \
   /tmp/mmcgrath/hits-hash 50220423 0 10 1 0 0 100 100
</pre>

              <p>Here are some command-line invocations exercising some of the patterns
              discussed in the <a href="./Lin_Schatz_MLG2010.pdf">Lin &amp; Schatz
              paper</a>:</p>
              <pre>
# hash partitioning, basic implementation, +IMC
hadoop jar cloud9.jar edu.umd.cloud9.example.hits.HubsAndAuthorities \
   /tmp/mmcgrath/hits-hash 50220423 0 10 0 1 0 500 500

# hash partitioning, Schimmy implementation, +IMC
hadoop jar cloud9.jar edu.umd.cloud9.example.hits.HubsAndAuthoritiesSchimmy \
   /tmp/mmcgrath/hits-hash 50220423 0 10 0 1 0 500 500

# range partitioning, basic implementation, +IMC
hadoop jar cloud9.jar edu.umd.cloud9.example.hits.HubsAndAuthorities \
   /tmp/mmcgrath/hits-range 50220423 0 10 0 1 1 500 500

# range partitioning, Schimmy implementation, +IMC
hadoop jar cloud9.jar edu.umd.cloud9.example.hits.HubsAndAuthoritiesSchimmy \
   /tmp/mmcgrath/PageRank-range 50220423 0 10 0 1 1 500 500
</pre>

              <p>Please keep in mind that if you wish to perform experiments on the
              ClueWeb09 data or a graph of similar size and structure, you will probably
              want to make use of the stoplist feature in the data formatting phase in
              order to prevent the reducers in <code>AFormatterWG</code> and
              <code>HubsAndAuthorities</code> from crunching away forever on the few
              nodes of extremely high in-degree. In experiments run on segment 1 of the
              ClueWeb09 data, I found it useful to ignore all nodes with in-degree
              greater than 100,000. The textfile containing these nodeIDs can be found
              <a href="./toobig.txt">here</a>. The Wikipedia graph, however, was small
              enough to render stoplists unneccessary. Your milage may vary based on the
              size and power of your cluster.</p>
            </div>
          </div><!--- END MAIN CONTENT HERE -->
        </div><!-- main END -->

        <div class="fixed"></div>
      </div><!-- content END -->
      <!-- footer START -->

      <div id="footer">
        <div id="copyright">
          Last updated: <script type="text/javascript">
//<![CDATA[
          <!--//
          document.write(document.lastModified);
          //-->
          //]]>
          </script>
        </div>

        <div id="themeinfo">
          Adapted from a WordPress Theme by <a href=
          "http://www.neoease.com/">NeoEase</a>. Valid <a href=
          "http://validator.w3.org/check?uri=referer">XHTML 1.1</a> and <a href=
          "http://jigsaw.w3.org/css-validator/check/referer?profile=css3">CSS 3</a>.
        </div>
      </div><!-- footer END -->
    </div><!-- container END -->
  </div><!-- wrap END -->
</body>
</html>
